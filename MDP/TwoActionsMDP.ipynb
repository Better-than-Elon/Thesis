{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hZSK6-8ArHl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import normalize\n",
    "from itertools import count\n",
    "from torch.autograd import Variable\n",
    "from environments.base_environment import OneHotEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tgD0T5gQECso"
   },
   "outputs": [],
   "source": [
    "class AverageRewardMDP_Agent(OneHotEnv):\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = 0.3\n",
    "        \n",
    "        self.num_states = 9\n",
    "        self.num_actions = 2\n",
    "        \n",
    "        #Env params\n",
    "        self.start_state = 0\n",
    "        self.current_state = 0\n",
    "        self.reward_scale_factor = None\n",
    "        self.P = torch.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        self.R = torch.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        # adding connections from node i to i+1\n",
    "        for s in range(self.num_states - 1):\n",
    "            self.P[s, 0, s + 1] = 1\n",
    "            self.P[s, 1, s + 1] = 1\n",
    "        # connection from N-1th to 0th node\n",
    "        self.P[8, 0, 0] = 1; self.P[8, 1, 0] = 1\n",
    "        # removing the connection from 4th to 5th node\n",
    "        self.P[4, 0, 5] = 0; self.P[4, 1, 5] = 0\n",
    "        # connection from 4th to 0th node\n",
    "        self.P[4, 0, 0] = 1; self.P[4, 1, 0] = 1\n",
    "        # action 1 in node 0 should not lead to 1, but 5\n",
    "        self.P[0, 1, 1] = 0\n",
    "        self.P[0, 1, 5] = 1\n",
    "        # rewards for going from 0 to 1 and 8 to 0\n",
    "        self.R[0, 0, 1] = 1\n",
    "        self.R[8, 0, 0] = 2; self.R[8, 1, 0] = 2\n",
    "        \n",
    "        self.random_seed = 2023\n",
    "        self.rand_generator = np.random.RandomState(self.random_seed)\n",
    "\n",
    "        self.start_state = self.rand_generator.choice(self.num_states)\n",
    "        self.reward_obs_term = [0.0, None, False]\n",
    "        \n",
    "        self.R_matrix = torch.zeros(self.num_states,self.num_actions)\n",
    "        possible_transitions = self.R*self.P\n",
    "        for i, actions in enumerate(possible_transitions):\n",
    "            self.R_matrix[i] = actions.sum(dim=1)\n",
    "        \n",
    "        self.P = self.P.reshape(self.num_actions*self.num_states, self.num_states)\n",
    "        #agent params\n",
    "        self.policy = torch.zeros(self.num_states, self.num_actions, requires_grad=True)\n",
    "        self.policy_optimizer = torch.optim.Adam([self.policy], lr=0.001)\n",
    "        \n",
    "        self.T = torch.zeros(self.num_states, requires_grad=True)\n",
    "        self.T_optimizer = torch.optim.Adam([self.T], lr=0.001)\n",
    "        \n",
    "        self.potential = torch.zeros(self.num_states, requires_grad=True)\n",
    "        self.potential_optimizer = torch.optim.Adam([self.potential], lr=0.001)\n",
    "        \n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "        self.psm = nn.Softmax(dim=1)\n",
    "\n",
    "    \n",
    "    def train(self, rounds=10):\n",
    "        for step in range(rounds):\n",
    "            self.policy.requires_grad_(True)\n",
    "            self.T.requires_grad_(True)\n",
    "            self.potential.requires_grad_(False)\n",
    "            for g in range(1):\n",
    "                \n",
    "                objective = self.R_matrix.flatten()*self.sm(self.policy.flatten())#self.psm(self.policy).flatten()\n",
    "                entropy = -torch.log(self.sm(self.policy.flatten()))*self.sm(self.policy.flatten())\n",
    "                t_part = self.potential*self.sm(self.T)\n",
    "                p_part = torch.mm(self.P, self.potential.reshape(self.num_states,1)).reshape(self.num_states,self.num_actions).flatten()*self.sm(self.policy.flatten())\n",
    "                \n",
    "                \n",
    "                #print('objective   ', objective.shape)\n",
    "                #print('f_constraint',f_constraint.shape)\n",
    "                #print('p_constraint', p_constraint.shape)\n",
    "                \n",
    "                policy_loss = -objective.sum() + t_part.sum() - p_part.sum()# - 10*entropy.sum()\n",
    "               # policy_loss = -(self.R_matrix.flatten()*self.psm(self.policy.flatten())).sum() \\\n",
    "               #               -(torch.mm(self.P.reshape(self.num_actions*self.num_states, self.num_states), self.potential.reshape(self.num_states,1))*self.sm(self.policy.flatten())).sum()\n",
    "                \n",
    "                #print('Reward and policy:', (self.R_matrix*self.sm(self.policy)).shape) torch.Size([9, 2])\n",
    "                #print('Potential and T:', (self.potential*self.sm(self.T)).shape) torch.Size([9])\n",
    "                #print('P and Potential:', (torch.mm(self.P.reshape(self.num_states*self.num_actions, self.num_states), self.potential.reshape(self.num_states,1)).shape)) torch.Size([18, 1])\n",
    "                #kmd\n",
    "                #policy_loss = -policy_loss\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                self.T_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "                self.T_optimizer.step()\n",
    "            \n",
    "            self.policy.requires_grad_(False)\n",
    "            self.T.requires_grad_(False)\n",
    "            self.potential.requires_grad_(True)\n",
    "            for d in range(1):\n",
    "                \n",
    "                t_part = self.potential*self.sm(self.T)\n",
    "                p_part = torch.mm(self.P, self.potential.reshape(self.num_states,1)).reshape(self.num_states,self.num_actions).flatten()*self.sm(self.policy.flatten())\n",
    "                f_loss = t_part.sum() - p_part.sum()\n",
    "                f_loss = -f_loss\n",
    "                \n",
    "                self.potential_optimizer.zero_grad()\n",
    "                f_loss.backward() \n",
    "                self.potential_optimizer.step()\n",
    "            if step %1000 ==0:       \n",
    "                print('Policy loss: ',policy_loss.item())\n",
    "                print('Potentials loss: ', f_loss.item())\n",
    "                print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOcVEPUNJqc9",
    "outputId": "3338d603-7aa3-47e9-88d3-e72580021824",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss:  0.2777777910232544\n",
      "Potentials loss:  -0.0\n",
      "----------------------------------\n",
      "Policy loss:  0.08594746142625809\n",
      "Potentials loss:  -0.009422238916158676\n",
      "----------------------------------\n",
      "Policy loss:  0.0379662811756134\n",
      "Potentials loss:  -0.0010499954223632812\n",
      "----------------------------------\n",
      "Policy loss:  -0.0038937460631132126\n",
      "Potentials loss:  0.02148124761879444\n",
      "----------------------------------\n",
      "Policy loss:  -0.007359161972999573\n",
      "Potentials loss:  0.01724955439567566\n",
      "----------------------------------\n",
      "Policy loss:  -0.0012164413928985596\n",
      "Potentials loss:  0.008312709629535675\n",
      "----------------------------------\n",
      "Policy loss:  0.032965198159217834\n",
      "Potentials loss:  -0.02733531966805458\n",
      "----------------------------------\n",
      "Policy loss:  -0.03129586577415466\n",
      "Potentials loss:  0.035596683621406555\n",
      "----------------------------------\n",
      "Policy loss:  -0.010031193494796753\n",
      "Potentials loss:  0.012690022587776184\n",
      "----------------------------------\n",
      "Policy loss:  0.0554366409778595\n",
      "Potentials loss:  -0.05388052761554718\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = AverageRewardMDP_Agent()\n",
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.6785,  1.9867],\n",
       "         [ 1.9600,  1.9600],\n",
       "         [ 1.9600,  1.9600],\n",
       "         [ 1.9600,  1.9600],\n",
       "         [ 1.0222,  1.0222],\n",
       "         [ 1.9600,  1.9600],\n",
       "         [ 1.9600,  1.9600],\n",
       "         [ 1.9600,  1.9600],\n",
       "         [-4.1230, -4.1230]]),\n",
       " tensor([[0.0035, 0.9965],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.5000]]),\n",
       " tensor([0.0003, 0.0743, 0.0724, 0.0724, 0.0724, 0.0724, 0.0724, 0.0724, 0.0283,\n",
       "         0.0283, 0.0724, 0.0724, 0.0724, 0.0724, 0.0724, 0.0724, 0.0002, 0.0002]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the results without using f and P in objectife\n",
    "agent.policy, agent.psm(agent.policy), agent.sm(agent.policy.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1   reward: 0.0   next_state: 5\n",
      "action: 1   reward: 0.0   next_state: 6\n",
      "action: 1   reward: 0.0   next_state: 7\n",
      "action: 0   reward: 0.0   next_state: 8\n",
      "action: 0   reward: 2.0   next_state: 0\n",
      "action: 1   reward: 0.0   next_state: 5\n",
      "action: 0   reward: 0.0   next_state: 6\n",
      "action: 0   reward: 0.0   next_state: 7\n",
      "action: 1   reward: 0.0   next_state: 8\n",
      "action: 0   reward: 2.0   next_state: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    action = np.random.choice(agent.num_actions)\n",
    "    obs = agent.env_step(action)\n",
    "    print('action:',action, '  reward:',obs[0].item(), '  next_state:',np.argmax(obs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "am7aScFxTJQf",
    "outputId": "f4880f2d-2f54-4b6d-88fe-a646e2a9e243",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5785, -1.6616, 11.7198, 11.7198, 11.7198, -1.6616, 11.7198, 11.7198,\n",
       "        11.7198])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([76.7428, 48.8321, 47.9522, 47.9522, 47.9522, 48.8321, 47.9522, 47.9522,\n",
       "        47.9522], requires_grad=True)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.potential"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
