{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4hZSK6-8ArHl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import normalize\n",
    "from itertools import count\n",
    "from torch.autograd import Variable\n",
    "from environments.base_environment import OneHotEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "tgD0T5gQECso"
   },
   "outputs": [],
   "source": [
    "class RiverSwimMDP_Agent(OneHotEnv):\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = 0.3\n",
    "        \n",
    "        self.num_states = 6\n",
    "        self.num_actions = 2\n",
    "        \n",
    "        #Env params\n",
    "        self.start_state = 0\n",
    "        self.current_state = 0\n",
    "        self.reward_scale_factor = None\n",
    "        self.P = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        self.R = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        # adding connections from node i to i+1\n",
    "        for s in range(self.num_states):\n",
    "            if s == 0:\n",
    "                self.P[s, 0, s] = 1\n",
    "                self.P[s, 1, s] = 0.1\n",
    "                self.P[s, 1, s + 1] = 0.9\n",
    "                self.R[s, 0, s] = 5. / 1000.\n",
    "            elif s == self.num_states - 1:\n",
    "                self.P[s, 0, s - 1] = 1\n",
    "                self.P[s, 1, s - 1] = 0.05\n",
    "                self.P[s, 1, s] = 0.95\n",
    "                self.R[s, 1, s] = 1\n",
    "            else:\n",
    "                self.P[s, 0, s - 1] = 1\n",
    "                self.P[s, 1, s - 1] = 0.05\n",
    "                self.P[s, 1, s] = 0.05\n",
    "                self.P[s, 1, s + 1] = 0.9\n",
    "\n",
    "        self.random_seed = 2023\n",
    "        self.rand_generator = np.random.RandomState(self.random_seed)\n",
    "        self.start_state = self.rand_generator.choice(self.num_states)\n",
    "        self.reward_obs_term = [0.0, None, False]\n",
    "        \n",
    "        self.P_tensor = torch.FloatTensor(self.P)\n",
    "        self.R_tensor = torch.FloatTensor(self.R)\n",
    "        \n",
    "        possible_transitions = self.R_tensor*self.P_tensor\n",
    "        self.R_tensor = torch.zeros(self.num_states,self.num_actions)\n",
    "        for i, actions in enumerate(possible_transitions):\n",
    "            self.R_tensor[i] = actions.sum(dim=1)\n",
    "            \n",
    "        self.P_tensor = self.P_tensor.reshape(self.num_actions*self.num_states, self.num_states)\n",
    "        \n",
    "        #Agent params\n",
    "        self.policy = torch.zeros(self.num_states, self.num_actions, requires_grad=True)\n",
    "        self.policy_optimizer = torch.optim.Adam([self.policy], lr=0.001)\n",
    "        \n",
    "        self.T = torch.zeros(self.num_states, requires_grad=True)\n",
    "        self.T_optimizer = torch.optim.Adam([self.T], lr=0.001)\n",
    "        \n",
    "        self.potential = torch.zeros(self.num_states, requires_grad=True)\n",
    "        self.potential_optimizer = torch.optim.Adam([self.potential], lr=0.001)\n",
    "        \n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "        self.psm = nn.Softmax(dim=1)\n",
    "\n",
    "    \n",
    "    def train(self, rounds=10):\n",
    "        for step in range(rounds):\n",
    "            self.policy.requires_grad_(True)\n",
    "            self.T.requires_grad_(True)\n",
    "            self.potential.requires_grad_(False)\n",
    "            for g in range(10):\n",
    "                \n",
    "                objective = self.R_tensor.flatten()*self.sm(self.policy.flatten())\n",
    "                entropy = -torch.log(self.sm(self.policy.flatten()))*self.sm(self.policy.flatten())\n",
    "                t_part = self.potential*self.sm(self.T)\n",
    "                p_part = torch.mm(self.P_tensor, self.potential.reshape(self.num_states,1)).reshape(self.num_states,self.num_actions).flatten()*self.sm(self.policy.flatten())\n",
    "                \n",
    "#                 objective = self.R_tensor.flatten()*self.psm(self.policy).flatten()\n",
    "#                 entropy = -torch.log(self.psm(self.policy).flatten())*self.psm(self.policy).flatten()\n",
    "#                 t_part = self.potential*self.sm(self.T)\n",
    "#                 p_part = torch.mm(self.P_tensor, self.potential.reshape(self.num_states,1)).reshape(self.num_states,self.num_actions).flatten()*self.psm(self.policy).flatten()\n",
    "                \n",
    "                \n",
    "                #print('objective   ', objective.shape)\n",
    "                #print('f_constraint',f_constraint.shape)\n",
    "                #print('p_constraint', p_constraint.shape)\n",
    "                \n",
    "                policy_loss = -objective.sum() + t_part.sum() - p_part.sum() #- 10*entropy.sum()\n",
    "               # policy_loss = -(self.R_matrix.flatten()*self.psm(self.policy.flatten())).sum() \\\n",
    "               #               -(torch.mm(self.P.reshape(self.num_actions*self.num_states, self.num_states), self.potential.reshape(self.num_states,1))*self.sm(self.policy.flatten())).sum()\n",
    "                \n",
    "                #print('Reward and policy:', (self.R_matrix*self.sm(self.policy)).shape) torch.Size([9, 2])\n",
    "                #print('Potential and T:', (self.potential*self.sm(self.T)).shape) torch.Size([9])\n",
    "                #print('P and Potential:', (torch.mm(self.P.reshape(self.num_states*self.num_actions, self.num_states), self.potential.reshape(self.num_states,1)).shape)) torch.Size([18, 1])\n",
    "                #kmd\n",
    "                #policy_loss = -policy_loss\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                self.T_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "                self.T_optimizer.step()\n",
    "            \n",
    "            self.policy.requires_grad_(False)\n",
    "            self.T.requires_grad_(False)\n",
    "            self.potential.requires_grad_(True)\n",
    "            for d in range(1):\n",
    "                \n",
    "                t_part = self.potential*self.sm(self.T)\n",
    "                p_part = torch.mm(self.P_tensor, self.potential.reshape(self.num_states,1)).reshape(self.num_states,self.num_actions).flatten()*self.sm(self.policy.flatten())\n",
    "                \n",
    "                # t_part = self.potential*self.sm(self.T)\n",
    "                # p_part = torch.mm(self.P_tensor, self.potential.reshape(self.num_states,1)).reshape(self.num_states,self.num_actions).flatten()*self.psm(self.policy).flatten()\n",
    "                f_loss = t_part.sum() - p_part.sum()\n",
    "                f_loss = -f_loss\n",
    "                \n",
    "                self.potential_optimizer.zero_grad()\n",
    "                f_loss.backward() \n",
    "                self.potential_optimizer.step()\n",
    "            if step %100 ==0:       \n",
    "                print('Policy loss: ',policy_loss.item())\n",
    "                print('Potentials loss: ', f_loss.item())\n",
    "                rewards = agent.evaluate_policy()\n",
    "                print('Rewards', rewards)\n",
    "                print('----------------------------------')\n",
    "                \n",
    "    def evaluate_policy(self):\n",
    "        rewards = 0\n",
    "        self.env_start()\n",
    "        for i in range(100):\n",
    "            prob = self.sm(self.policy[self.current_state])\n",
    "            action = np.random.choice([0,1], p=prob.detach().numpy())\n",
    "            #action = self.policy[self.current_state].argmax()\n",
    "            obs = self.env_step(action)\n",
    "            rewards+=obs[0]\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RiverSwimMDP_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOcVEPUNJqc9",
    "outputId": "3338d603-7aa3-47e9-88d3-e72580021824",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss:  -0.08089937269687653\n",
      "Potentials loss:  -0.0\n",
      "Rewards 7.015\n",
      "----------------------------------\n",
      "Policy loss:  -0.5130094289779663\n",
      "Potentials loss:  0.00017523346468806267\n",
      "Rewards 49.04500000000001\n",
      "----------------------------------\n",
      "Policy loss:  -0.8290524482727051\n",
      "Potentials loss:  0.0018662847578525543\n",
      "Rewards 66.01499999999999\n",
      "----------------------------------\n",
      "Policy loss:  -0.9030272364616394\n",
      "Potentials loss:  0.0013418085873126984\n",
      "Rewards 53.03\n",
      "----------------------------------\n",
      "Policy loss:  -0.9287384748458862\n",
      "Potentials loss:  0.0038876160979270935\n",
      "Rewards 37.015\n",
      "----------------------------------\n",
      "Policy loss:  -0.9343671798706055\n",
      "Potentials loss:  -0.0018609948456287384\n",
      "Rewards 20.04\n",
      "----------------------------------\n",
      "Policy loss:  -0.954905092716217\n",
      "Potentials loss:  0.012672502547502518\n",
      "Rewards 13.085\n",
      "----------------------------------\n",
      "Policy loss:  -0.9480862021446228\n",
      "Potentials loss:  0.0029198825359344482\n",
      "Rewards 88.0\n",
      "----------------------------------\n",
      "Policy loss:  -0.9521186947822571\n",
      "Potentials loss:  0.005157746374607086\n",
      "Rewards 51.015\n",
      "----------------------------------\n",
      "Policy loss:  -0.9485558271408081\n",
      "Potentials loss:  0.00040986761450767517\n",
      "Rewards 41.035\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13079/1390286933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13079/3145426388.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, rounds)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0050, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.9500]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.R_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.3411, -4.3409],\n",
       "         [-4.3281, -4.3418],\n",
       "         [-4.3419, -4.3425],\n",
       "         [-4.3424, -4.3708],\n",
       "         [-4.3426, -4.3979],\n",
       "         [-4.3736,  4.3533]], requires_grad=True),\n",
       " tensor([[4.9995e-01, 5.0005e-01],\n",
       "         [5.0342e-01, 4.9658e-01],\n",
       "         [5.0014e-01, 4.9986e-01],\n",
       "         [5.0711e-01, 4.9289e-01],\n",
       "         [5.1383e-01, 4.8617e-01],\n",
       "         [1.6213e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the results without using f and P in objectife\n",
    "agent.policy, agent.psm(agent.policy), #agent.sm(agent.policy.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1   reward: 0.0   next_state: 5\n",
      "action: 1   reward: 0.0   next_state: 6\n",
      "action: 1   reward: 0.0   next_state: 7\n",
      "action: 0   reward: 0.0   next_state: 8\n",
      "action: 0   reward: 2.0   next_state: 0\n",
      "action: 1   reward: 0.0   next_state: 5\n",
      "action: 0   reward: 0.0   next_state: 6\n",
      "action: 0   reward: 0.0   next_state: 7\n",
      "action: 1   reward: 0.0   next_state: 8\n",
      "action: 0   reward: 2.0   next_state: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    action = np.random.choice(agent.num_actions)\n",
    "    obs = agent.env_step(action)\n",
    "    print('action:',action, '  reward:',obs[0].item(), '  next_state:',np.argmax(obs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "am7aScFxTJQf",
    "outputId": "f4880f2d-2f54-4b6d-88fe-a646e2a9e243",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5785, -1.6616, 11.7198, 11.7198, 11.7198, -1.6616, 11.7198, 11.7198,\n",
       "        11.7198])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([76.7428, 48.8321, 47.9522, 47.9522, 47.9522, 48.8321, 47.9522, 47.9522,\n",
       "        47.9522], requires_grad=True)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.potential"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
